{"Request":{"model":"gpt-4","messages":[{"role":"system","content":"Digest the article and output a description of the authors persona and writing style. Preserve some unique samples of the writing to help the LLM. Provide a list of similar well-known authors and highlight key differences. This description must be useful to help LLMs emulate the authors distinct style, perspective, and philosophical impulses."},{"role":"user","content":"Article: \"\"\"TL;DR - You can skip right to the [results]({{\u003c relref \"#results\" \u003e}}) for the code.\n\nA list that recently hit [Hashes.org](http://hashes.org), with 1 million records and a low crack rate, looked like an interesting target, given that the community had recovered less than 0.5% of the list. On taking a closer look, we find out why;\n\nOut of the box, only John the Ripper (JtR) Jumbo carries support for the XenForo hash scheme[^1] as a **dynamic hash**[^2].\n\n[^2]: https://github.com/magnumripper/JohnTheRipper/blob/bleeding-jumbo/doc/DYNAMIC\n[^1]: https://github.com/magnumripper/JohnTheRipper/blob/323f4babdefadfae99568474557dacc2a130d8b3/run/dynamic.conf#L945\n\n\nJtR’s dynamic hashes don’t run on OpenCL, and a benchmark on a single core resulted in 4MH/s, while during a run we hit around 80MH/s across 20 CPU Cores. (H/T Solar Designer for the correction.)\n\n{{\u003cimg \"jtrimg\" \"Around 4 MH/s from JtR in CPU mode.\" \u003e}}\n\nWhat we had was a pretense for writing a custom OpenCL kernel for Hashcat—perfect. Adding to the effort was the lack of documentation around Hashcat (and sparse code comments).\n\nMy general methodology was running Hashcat in single-hash mode against the kernel and using `printf()` debugging statements to understand what was going on under the hood. Hashcat's other kernels and its OpenSSL-style functions were definitely key to getting something working in a limited amount of time.\n\n{{\u003caside\u003e}}Compared to C on *nix, OpenCL development is exotic. If you have a solid environment for Hashcat work, let me know.\n{{\u003c/aside\u003e}}\n\n## Orientation\n\n`./hashcat/include/interface.h`[^3]\n: `enums` and values required to add a new hash type.\n\n`./hashcat/src/interface.c`[^4]\n: Blocks that describe each hash type. It contains strings for Hashcat’s help function, as well as various constants and hash-parsing functions.\n\n`./hashcat/kernels/`\n: Compiled OpenCL kernels. Hashcat compiles each OpenCL kernel at runtime and caches those kernels here.\n\n`./hashcat/OpenCL/`[^5]\n: OpenCL kernel sources (`.cl` files).\n\n[^3]: https://github.com/hashcat/hashcat/blob/master/include/interface.h\n[^4]: https://github.com/hashcat/hashcat/blob/master/src/interface.c\n[^5]: https://github.com/hashcat/hashcat/tree/master/OpenCL\n\n## Play-by-play\n\n{{\u003cbc \"Dr. Conrad Zimsky (Stanley Tucci), The Core (2003)\"\u003e}}\nlet me smoke a cigarette, and I'll tell you.\n{{\u003c/bc\u003e}}\n\n### Inspiration\nTo start with, we identify an OpenCL `a0` kernel (or set of kernels) that closely matches our workload and input type. For example, if your hash is salted, find a salted hash of the same type, and likewise if it is iterated.\n\nCopy it to its own kernel file, designated as `m[Hashcat Mode Flag]_a0-pure.cl`. Search/replace this source to fix up the mode function names. In this guide, I used `m01410_a0-pure.cl`[^m01410_a0-pure] as the donor kernel. I'll be using `m01415_a0-pure.cl` for the new mode `1415`.\n[^m01410_a0-pure]: https://github.com/hashcat/hashcat/blob/master/OpenCL/m01410_a0-pure.cl\n\n{{\u003caside wide\u003e}}Hashcat follows a loose naming convention for modes: `xxxyy`, where `xxx` denotes the family and `yy` is either `00` for unsalted, `10` for an appended salt, or `20` for a prepended salt. There are plenty of counter-examples, but this is the general scheme.{{\u003c/aside\u003e}}\n\n### Adding the mode\n\nNext up, add the new mode to Hashcat.\n\nWe add a new `enum` value `SHA256_PW_SHA256_SLT` to `interface.h` and set it equal to `1415`:\n\n{{\u003ccode \"Grabbing a new Hashcat `-m`ode flag, `1415`, for the XenForo mode.\" \u003e}}\n\t{{\u003chighlight c\u003e}}\nKERN_TYPE_SHA256_PWSLT = 1410,\nKERN_TYPE_SHA256_PW_SHA256_SLT = 1415,\nKERN_TYPE_SHA256_SLTPW = 1420,\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\n`HT_`* is CLI / Help-related Text and a new entry `HT_01415` is added to `interface.c`:\n\n{{\u003ccode \"Adding reference to our new XenForo mode in `interface.c`.\" \u003e}}\n\t{{\u003chighlight c\u003e}}\ncase 1411: return HT_01411;\ncase 1415: return HT_01415;\ncase 1420: return HT_01420;\nstatic const char *HT_01410 = \"sha256($pass.$salt)\";\nstatic const char *HT_01415 = \"sha256(sha256($pass).$salt)\";\nstatic const char *HT_01420 = \"sha256($salt.$pass)\";\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\nA test hash `ST_HASH_01415` plus its corresponding plaintext `ST_PASS_HASHCAT_PEANUT` are used for self-testing:\n\n{{\u003ccode \"Adding a test hash and plaintext to `interface.c`.\" \u003e}}\n\t{{\u003chighlight c\u003e}}\nstatic const char *ST_PASS_HASHCAT_PEANUT = \"peanut\";\n...\nstatic const char *ST_HASH_01415 = \"00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449\";\n...\ncase 1411: hashconfig-\u003ehash_type = HASH_TYPE_SHA256;\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\nAnd in the main hash configuration block, the values were lifted from `case 1410:`, `sha256($plain.$salt)`.\n\nAn important bit here is to set `kern_type` to the `enum` added to `interface.h`, in this case `KERN_TYPE_SHA256_PW_SHA256_SLT`. `parse_func` is a function pointer, set to `sha256s_parse_hash` - which will be invoked to parse the hash list. If your hash format can't be consumed by these functions exactly, it will be rejected. Hashcat has a robust set of these parsing functions, and you'll want to hunt through them in `interface.c`[^hcparse].  \n[^hcparse]: https://github.com/hashcat/hashcat/blob/dbbba1fbdf05403675ddbf7d3b36f42ab7b76f68/include/interface.h#L1509\n\n{{\u003caside\u003e}}`opti_type` does *something*. Reach out if you have some information about its semantics, and I'll update this article.{{\u003c/aside\u003e}}\n\n\n### Quick and dirty test\n\nAfter a `make clean \u0026\u0026 make`, Hashcat should list your new type. Benchmark mode won't be useful for testing, as the `a0` kernels aren’t used here. Running a simple single-hash test against the new kernel will cause Hashcat to build it from the `OpenCL` directory to the `kernels` directory. This is when you'll receive compile-time errors. \n{{\u003caside\u003e}}There's probably a way to directly invoke the OpenCL build toolchain to speed this up. Let me know if you know.{{\u003c/aside\u003e}}\n\nIn this case, I had known hash/salt/plaintext values (recoverable from JtR and publicly listed). These are essential. You'll find out how to use the raw results of intermediate steps in the scheme for debugging in the next section.\n\nI used this example for testing:\n\n{{\u003ccode \"Sample XenForo hash, salt, and plaintext tuple - for the plaintext 'peanut'. The format is `[hash]:[salt]:[plaintext]`.\"\u003e}}\n{{\u003chighlight text\u003e}}\n00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449:peanut\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\nSteps from the command line:\n\n{{\u003ccode \"Re-create the sample hash for 'peanut' using `shasum`. This confirms our scheme.\"\u003e}}\n{{\u003chighlight text\u003e}}\n## echo -n peanut | shasum -a 256\n5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06 –\n## echo -n 5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449 | shasum -a 256\n00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa –\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\n{{\u003crag\u003e}}\nReviewing; with the `$salt` value `b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449` where `sha256('peanut')` is `5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06` and `sha256(sha256('peanut') . $salt)` is `00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa`. As expected by `sha256(sha256($pass).$salt)`.\n{{\u003c/rag\u003e}}\n\nHashcat test:\n\n{{\u003ccode \"Testing our new `-m`ode 1415's `-a 0` kernel. `peanut.txt` contains the word 'peanut'.\"\u003e}}\n{{\u003chighlight text\u003e}}\n## ./hashcat -m 1415 -a 0 “00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449” peanut.txt\n\nhashcat (v4.0.1-123-g2095e27d+) starting...\nOpenCL Platform #1: Apple\n\n=========================\n\n* Device #1: Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz, skipped.\n* Device #2: Intel(R) HD Graphics 530, 384/1536 MB allocatable, 24MCU\n* Device #3: AMD Radeon Pro 460 Compute Engine, 1024/4096 MB allocatable, 16MCU\n\n...\nWatchdog: Temperature abort trigger disabled.\n* Device #2: ATTENTION! OpenCL kernel self-test failed.\nYour device driver installation is probably broken.\n\nSee also: https://hashcat.net/faq/wrongdriver\n* Device #3: ATTENTION! OpenCL kernel self-test failed.\nYour device driver installation is probably broken.\n\nSee also: https://hashcat.net/faq/wrongdriver\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\nWe now have a custom Hashcat build that lists our new type. Awesome. Let’s get these self-tests to pass with a single hash.\n\n### Kicking out the jams\n\n{{\u003cbc \"MC5, Kick Out the Jams (1969)\"\u003e}}\nYou gotta have it, baby, you can't do without\n\nOh, when you get that feeling you gotta sock 'em out\n{{\u003c/bc\u003e}}\n\nThe majority of the work on an `a0` kernel will be focused on these two functions:\n\n{{\u003caside\u003e}}We're using the donor kernel `m01410`, and we've renamed all the `1410`'s to `1415`'s. {{\u003c/aside\u003e}}\n\n`__kernel void m01415_mxx`\n: Invoked to handle cracking multiple target hashes at once. We can ignore this during testing.\n\n`__kernel void m01415_sxx`\n: Invoked in single-hash mode (which is how we're testing). We can focus our development effort here.\n\nLet's take a walk through the donor kernel's `m01410_sxx` function:\n\n{{\u003ccode \"The first block of the `m01410_sxx` function.\" \u003e}}\n{{\u003chighlight c\u003e}}/**\n* digest\n*/\nconst u32 search[4] =\n{\n\tdigests_buf[digests_offset].digest_buf[DGST_R0],\n\tdigests_buf[digests_offset].digest_buf[DGST_R1],\n\tdigests_buf[digests_offset].digest_buf[DGST_R2],\n\tdigests_buf[digests_offset].digest_buf[DGST_R3]\n};{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\nIn this kernel, Hashcat doesn't bother comparing all the bytes in the hash buffer — a basic optimization technique. It's unlikely to collide with a human-meaningful candidate as selected by Hashcat, even when doing partial matches. The bytes are configured in the `dgst_pos`* values of the `hashconfig` `struct`:\n\n{{\u003ccode \"Our mode's `dgst_pos`* values from `interface.c`.\" \u003e}}\n{{\u003chighlight c\u003e}}\nhashconfig-\u003edgst_pos0 = 3;\nhashconfig-\u003edgst_pos1 = 7;\nhashconfig-\u003edgst_pos2 = 2;\nhashconfig-\u003edgst_pos3 = 6;\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\n\nUse values from another kernel that has the same final hash round as your target.\n\n{{\u003ccode \"`m01410_sxx` preps `s[]` for `sha256` so it can be appended before finalization.\" \u003e}}\n{{\u003chighlight c\u003e}}/**\n* base\n*/\nCOPY_PW (pws[gid]);\nconst u32 salt_len = salt_bufs[salt_pos].salt_len;\nu32 s[64] = { 0 };\nfor (int i = 0, idx = 0; i \u003c salt_len; i += 4, idx += 1)\n{\n\ts[idx] = swap32_S (salt_bufs[salt_pos].salt_buf[idx]);\n}{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}} \n\nThe kernel prepares the salt buffer once and carries out operations usually performed by the Hashcat `sha256` library's `sha256_update_swap` function.\n\nNow is a good time to browse through the methods in `inc_hash_sha256.cl`[^inc_hash_sha256]. These aren't documented in any way, and while they resemble other low-level C crypto implementations, some things have been changed to permit more efficient cracking. It helps to select methods out of this library, and those in its hash family (e.g., `sha512`, `sha1`), to understand how these functions can be used in various contexts.[^shaex]\n\n[^inc_hash_sha256]: https://github.com/hashcat/hashcat/blob/master/OpenCL/inc_hash_sha256.cl\n\n[^shaex]: https://github.com/hashcat/hashcat/search?q=inc_hash_sha256.cl\n\nThe function's main loop permutes the password root candidate `tmp.i` according to rules identified in the `rules_buf` array. It then establishes a `sha256` context, updates the context with the password candidate buffer, updates it (minus the swap already performed) with the salt, and then finalizes the hash. It then compares the bytes outlined in `hashconfig-\u003edgst_pos`* for a match.\n\nBecause our source function is `sha256($plain.$salt)` and our target is `sha256(sha256($plain).$salt)`, we know we need to add a round of `sha256` to the plaintext before updating the final `sha256` context, `ctx`.\n\nIn my case, it took a few attempts to get the bit shifting / byte ordering correct. Here's what I did:\n\nWe know, ultimately, that `w0`, `w1`, `w2`, and `w3` needed to hold the ASCII representation of the `sha256` hash, using our example 'peanut':\n\n\nUsing these hardcoded values, Hashcat passes the self-test... Progress!\n\nWe can play with byte ordering and bit shifting until `w0[0]` holds `0x35353039` (which, unless you're shifting on a regular basis, isn't natural):\n\nYou can debug the kernel by alternating `rm -rf ./hashcat/kernels` with your Hashcat single-hash test command *e.g.* `./hashcat -m 1415 -a 0 “00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449” peanut.txt`. This is how I nailed down byte ordering and conversion.\n\nWith the right values in `w0`, `w1`, `w2`, and `w3`, we can move the *`_sxx` changes over to *`_mxx`. Just reapply your changes there while making sure the final comparison function is `COMPARE_M` instead of `COMPARE_S`.\n\n## Results\n\n{{\u003cimg \"hcresult\" \"Running our new XenForo kernel.\" \u003e}}\n\nOn an eight Nvidia 1080 GPU cracking rig from [sagitta](https://sagitta.pw/hardware/gpu-compute-nodes/brutalis/), we hit 605 MH/s with this unoptimized kernel — 7 times faster than CPU mode.\n\nI'll need to wait for another rainy day (and a corpus with a shorter salt) to hack on an optimized kernel.\n\nYou can find the final implementation at this [GitHub Gist](https://gist.github.com/tbiehn/28531fdb324baaeaa2960aae1b9b36ab).\"\"\""}],"max_tokens":4000},"Response":"The author's persona is that of an expert in cybersecurity and hash cracking, possessing a deep understanding of the inner workings of Hashcat and John the Ripper. The writing style is technical, detailed, and explanatory, walking the reader through a step-by-step process of creating a custom OpenCL kernel for Hashcat.\n\nUnique samples of the writing include:\n- \"We now have a custom Hashcat build that lists our new type. Awesome. Let’s get these self-tests to pass with a single hash.\"\n- \"In my case, it took a few attempts to get the bit shifting / byte ordering correct.\"\n- \"You can debug the kernel by alternating rm -rf ./hashcat/kernels with your Hashcat single-hash test command.\"\n\nSimilar well-known authors might include Bruce Schneier and Brian Krebs. The key difference is that the author focuses on the practical aspects of hash cracking and the implementation process, while Schneier and Krebs often write about broader cybersecurity topics and their implications.\n\nTo emulate the author's distinct style, perspective, and philosophical impulses, an LLM should focus on providing clear, technically detailed explanations that walk the reader through the implementation process step by step. Additionally, the LLM should exhibit expertise in the field of hash cracking and cybersecurity and maintain a tone that is both authoritative and approachable.","Identifier":"{\"article\":\"TL;DR - You can skip right to the [results]({{\\u003c relref \\\"#results\\\" \\u003e}}) for the code.\\n\\nA list that recently hit [Hashes.org](http://hashes.org), with 1 million records and a low crack rate, looked like an interesting target, given that the community had recovered less than 0.5% of the list. On taking a closer look, we find out why;\\n\\nOut of the box, only John the Ripper (JtR) Jumbo carries support for the XenForo hash scheme[^1] as a **dynamic hash**[^2].\\n\\n[^2]: https://github.com/magnumripper/JohnTheRipper/blob/bleeding-jumbo/doc/DYNAMIC\\n[^1]: https://github.com/magnumripper/JohnTheRipper/blob/323f4babdefadfae99568474557dacc2a130d8b3/run/dynamic.conf#L945\\n\\n\\nJtR’s dynamic hashes don’t run on OpenCL, and a benchmark on a single core resulted in 4MH/s, while during a run we hit around 80MH/s across 20 CPU Cores. (H/T Solar Designer for the correction.)\\n\\n{{\\u003cimg \\\"jtrimg\\\" \\\"Around 4 MH/s from JtR in CPU mode.\\\" \\u003e}}\\n\\nWhat we had was a pretense for writing a custom OpenCL kernel for Hashcat—perfect. Adding to the effort was the lack of documentation around Hashcat (and sparse code comments).\\n\\nMy general methodology was running Hashcat in single-hash mode against the kernel and using `printf()` debugging statements to understand what was going on under the hood. Hashcat's other kernels and its OpenSSL-style functions were definitely key to getting something working in a limited amount of time.\\n\\n{{\\u003caside\\u003e}}Compared to C on *nix, OpenCL development is exotic. If you have a solid environment for Hashcat work, let me know.\\n{{\\u003c/aside\\u003e}}\\n\\n## Orientation\\n\\n`./hashcat/include/interface.h`[^3]\\n: `enums` and values required to add a new hash type.\\n\\n`./hashcat/src/interface.c`[^4]\\n: Blocks that describe each hash type. It contains strings for Hashcat’s help function, as well as various constants and hash-parsing functions.\\n\\n`./hashcat/kernels/`\\n: Compiled OpenCL kernels. Hashcat compiles each OpenCL kernel at runtime and caches those kernels here.\\n\\n`./hashcat/OpenCL/`[^5]\\n: OpenCL kernel sources (`.cl` files).\\n\\n[^3]: https://github.com/hashcat/hashcat/blob/master/include/interface.h\\n[^4]: https://github.com/hashcat/hashcat/blob/master/src/interface.c\\n[^5]: https://github.com/hashcat/hashcat/tree/master/OpenCL\\n\\n## Play-by-play\\n\\n{{\\u003cbc \\\"Dr. Conrad Zimsky (Stanley Tucci), The Core (2003)\\\"\\u003e}}\\nlet me smoke a cigarette, and I'll tell you.\\n{{\\u003c/bc\\u003e}}\\n\\n### Inspiration\\nTo start with, we identify an OpenCL `a0` kernel (or set of kernels) that closely matches our workload and input type. For example, if your hash is salted, find a salted hash of the same type, and likewise if it is iterated.\\n\\nCopy it to its own kernel file, designated as `m[Hashcat Mode Flag]_a0-pure.cl`. Search/replace this source to fix up the mode function names. In this guide, I used `m01410_a0-pure.cl`[^m01410_a0-pure] as the donor kernel. I'll be using `m01415_a0-pure.cl` for the new mode `1415`.\\n[^m01410_a0-pure]: https://github.com/hashcat/hashcat/blob/master/OpenCL/m01410_a0-pure.cl\\n\\n{{\\u003caside wide\\u003e}}Hashcat follows a loose naming convention for modes: `xxxyy`, where `xxx` denotes the family and `yy` is either `00` for unsalted, `10` for an appended salt, or `20` for a prepended salt. There are plenty of counter-examples, but this is the general scheme.{{\\u003c/aside\\u003e}}\\n\\n### Adding the mode\\n\\nNext up, add the new mode to Hashcat.\\n\\nWe add a new `enum` value `SHA256_PW_SHA256_SLT` to `interface.h` and set it equal to `1415`:\\n\\n{{\\u003ccode \\\"Grabbing a new Hashcat `-m`ode flag, `1415`, for the XenForo mode.\\\" \\u003e}}\\n\\t{{\\u003chighlight c\\u003e}}\\nKERN_TYPE_SHA256_PWSLT = 1410,\\nKERN_TYPE_SHA256_PW_SHA256_SLT = 1415,\\nKERN_TYPE_SHA256_SLTPW = 1420,\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\n`HT_`* is CLI / Help-related Text and a new entry `HT_01415` is added to `interface.c`:\\n\\n{{\\u003ccode \\\"Adding reference to our new XenForo mode in `interface.c`.\\\" \\u003e}}\\n\\t{{\\u003chighlight c\\u003e}}\\ncase 1411: return HT_01411;\\ncase 1415: return HT_01415;\\ncase 1420: return HT_01420;\\nstatic const char *HT_01410 = \\\"sha256($pass.$salt)\\\";\\nstatic const char *HT_01415 = \\\"sha256(sha256($pass).$salt)\\\";\\nstatic const char *HT_01420 = \\\"sha256($salt.$pass)\\\";\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\nA test hash `ST_HASH_01415` plus its corresponding plaintext `ST_PASS_HASHCAT_PEANUT` are used for self-testing:\\n\\n{{\\u003ccode \\\"Adding a test hash and plaintext to `interface.c`.\\\" \\u003e}}\\n\\t{{\\u003chighlight c\\u003e}}\\nstatic const char *ST_PASS_HASHCAT_PEANUT = \\\"peanut\\\";\\n…\\nstatic const char *ST_HASH_01415 = \\\"00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449\\\";\\n…\\ncase 1411: hashconfig-\\u003ehash_type = HASH_TYPE_SHA256;\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\nAnd in the main hash configuration block, the values were lifted from `case 1410:`, `sha256($plain.$salt)`.\\n\\nAn important bit here is to set `kern_type` to the `enum` added to `interface.h`, in this case `KERN_TYPE_SHA256_PW_SHA256_SLT`. `parse_func` is a function pointer, set to `sha256s_parse_hash` - which will be invoked to parse the hash list. If your hash format can't be consumed by these functions exactly, it will be rejected. Hashcat has a robust set of these parsing functions, and you'll want to hunt through them in `interface.c`[^hcparse].  \\n[^hcparse]: https://github.com/hashcat/hashcat/blob/dbbba1fbdf05403675ddbf7d3b36f42ab7b76f68/include/interface.h#L1509\\n\\n{{\\u003caside\\u003e}}`opti_type` does *something*. Reach out if you have some information about its semantics, and I'll update this article.{{\\u003c/aside\\u003e}}\\n\\n\\n### Quick and dirty test\\n\\nAfter a `make clean \\u0026\\u0026 make`, Hashcat should list your new type. Benchmark mode won't be useful for testing, as the `a0` kernels aren’t used here. Running a simple single-hash test against the new kernel will cause Hashcat to build it from the `OpenCL` directory to the `kernels` directory. This is when you'll receive compile-time errors. \\n{{\\u003caside\\u003e}}There's probably a way to directly invoke the OpenCL build toolchain to speed this up. Let me know if you know.{{\\u003c/aside\\u003e}}\\n\\nIn this case, I had known hash/salt/plaintext values (recoverable from JtR and publicly listed). These are essential. You'll find out how to use the raw results of intermediate steps in the scheme for debugging in the next section.\\n\\nI used this example for testing:\\n\\n{{\\u003ccode \\\"Sample XenForo hash, salt, and plaintext tuple - for the plaintext 'peanut'. The format is `[hash]:[salt]:[plaintext]`.\\\"\\u003e}}\\n{{\\u003chighlight text\\u003e}}\\n00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449:peanut\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\nSteps from the command line:\\n\\n{{\\u003ccode \\\"Re-create the sample hash for 'peanut' using `shasum`. This confirms our scheme.\\\"\\u003e}}\\n{{\\u003chighlight text\\u003e}}\\n## echo -n peanut | shasum -a 256\\n5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06 –\\n## echo -n 5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449 | shasum -a 256\\n00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa –\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\n{{\\u003crag\\u003e}}\\nReviewing; with the `$salt` value `b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449` where `sha256('peanut')` is `5509840d0873adb0405588821197a8634501293486c601ca51e14063abe25d06` and `sha256(sha256('peanut') . $salt)` is `00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa`. As expected by `sha256(sha256($pass).$salt)`.\\n{{\\u003c/rag\\u003e}}\\n\\nHashcat test:\\n\\n{{\\u003ccode \\\"Testing our new `-m`ode 1415's `-a 0` kernel. `peanut.txt` contains the word 'peanut'.\\\"\\u003e}}\\n{{\\u003chighlight text\\u003e}}\\n## ./hashcat -m 1415 -a 0 “00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449” peanut.txt\\n\\nhashcat (v4.0.1-123-g2095e27d+) starting…\\nOpenCL Platform #1: Apple\\n\\n=========================\\n\\n* Device #1: Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz, skipped.\\n* Device #2: Intel(R) HD Graphics 530, 384/1536 MB allocatable, 24MCU\\n* Device #3: AMD Radeon Pro 460 Compute Engine, 1024/4096 MB allocatable, 16MCU\\n\\n…\\nWatchdog: Temperature abort trigger disabled.\\n* Device #2: ATTENTION! OpenCL kernel self-test failed.\\nYour device driver installation is probably broken.\\n\\nSee also: https://hashcat.net/faq/wrongdriver\\n* Device #3: ATTENTION! OpenCL kernel self-test failed.\\nYour device driver installation is probably broken.\\n\\nSee also: https://hashcat.net/faq/wrongdriver\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\nWe now have a custom Hashcat build that lists our new type. Awesome. Let’s get these self-tests to pass with a single hash.\\n\\n### Kicking out the jams\\n\\n{{\\u003cbc \\\"MC5, Kick Out the Jams (1969)\\\"\\u003e}}\\nYou gotta have it, baby, you can't do without\\n\\nOh, when you get that feeling you gotta sock 'em out\\n{{\\u003c/bc\\u003e}}\\n\\nThe majority of the work on an `a0` kernel will be focused on these two functions:\\n\\n{{\\u003caside\\u003e}}We're using the donor kernel `m01410`, and we've renamed all the `1410`'s to `1415`'s. {{\\u003c/aside\\u003e}}\\n\\n`__kernel void m01415_mxx`\\n: Invoked to handle cracking multiple target hashes at once. We can ignore this during testing.\\n\\n`__kernel void m01415_sxx`\\n: Invoked in single-hash mode (which is how we're testing). We can focus our development effort here.\\n\\nLet's take a walk through the donor kernel's `m01410_sxx` function:\\n\\n{{\\u003ccode \\\"The first block of the `m01410_sxx` function.\\\" \\u003e}}\\n{{\\u003chighlight c\\u003e}}/**\\n* digest\\n*/\\nconst u32 search[4] =\\n{\\n\\tdigests_buf[digests_offset].digest_buf[DGST_R0],\\n\\tdigests_buf[digests_offset].digest_buf[DGST_R1],\\n\\tdigests_buf[digests_offset].digest_buf[DGST_R2],\\n\\tdigests_buf[digests_offset].digest_buf[DGST_R3]\\n};{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\nIn this kernel, Hashcat doesn't bother comparing all the bytes in the hash buffer — a basic optimization technique. It's unlikely to collide with a human-meaningful candidate as selected by Hashcat, even when doing partial matches. The bytes are configured in the `dgst_pos`* values of the `hashconfig` `struct`:\\n\\n{{\\u003ccode \\\"Our mode's `dgst_pos`* values from `interface.c`.\\\" \\u003e}}\\n{{\\u003chighlight c\\u003e}}\\nhashconfig-\\u003edgst_pos0 = 3;\\nhashconfig-\\u003edgst_pos1 = 7;\\nhashconfig-\\u003edgst_pos2 = 2;\\nhashconfig-\\u003edgst_pos3 = 6;\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\n\\nUse values from another kernel that has the same final hash round as your target.\\n\\n{{\\u003ccode \\\"`m01410_sxx` preps `s[]` for `sha256` so it can be appended before finalization.\\\" \\u003e}}\\n{{\\u003chighlight c\\u003e}}/**\\n* base\\n*/\\nCOPY_PW (pws[gid]);\\nconst u32 salt_len = salt_bufs[salt_pos].salt_len;\\nu32 s[64] = { 0 };\\nfor (int i = 0, idx = 0; i \\u003c salt_len; i += 4, idx += 1)\\n{\\n\\ts[idx] = swap32_S (salt_bufs[salt_pos].salt_buf[idx]);\\n}{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}} \\n\\nThe kernel prepares the salt buffer once and carries out operations usually performed by the Hashcat `sha256` library's `sha256_update_swap` function.\\n\\nNow is a good time to browse through the methods in `inc_hash_sha256.cl`[^inc_hash_sha256]. These aren't documented in any way, and while they resemble other low-level C crypto implementations, some things have been changed to permit more efficient cracking. It helps to select methods out of this library, and those in its hash family (e.g., `sha512`, `sha1`), to understand how these functions can be used in various contexts.[^shaex]\\n\\n[^inc_hash_sha256]: https://github.com/hashcat/hashcat/blob/master/OpenCL/inc_hash_sha256.cl\\n\\n[^shaex]: https://github.com/hashcat/hashcat/search?q=inc_hash_sha256.cl\\n\\nThe function's main loop permutes the password root candidate `tmp.i` according to rules identified in the `rules_buf` array. It then establishes a `sha256` context, updates the context with the password candidate buffer, updates it (minus the swap already performed) with the salt, and then finalizes the hash. It then compares the bytes outlined in `hashconfig-\\u003edgst_pos`* for a match.\\n\\nBecause our source function is `sha256($plain.$salt)` and our target is `sha256(sha256($plain).$salt)`, we know we need to add a round of `sha256` to the plaintext before updating the final `sha256` context, `ctx`.\\n\\nIn my case, it took a few attempts to get the bit shifting / byte ordering correct. Here's what I did:\\n\\nWe know, ultimately, that `w0`, `w1`, `w2`, and `w3` needed to hold the ASCII representation of the `sha256` hash, using our example 'peanut':\\n\\n\\nUsing these hardcoded values, Hashcat passes the self-test... Progress!\\n\\nWe can play with byte ordering and bit shifting until `w0[0]` holds `0x35353039` (which, unless you're shifting on a regular basis, isn't natural):\\n\\nYou can debug the kernel by alternating `rm -rf ./hashcat/kernels` with your Hashcat single-hash test command *e.g.* `./hashcat -m 1415 -a 0 “00050655d5d6b8a8c14d52e852ce930fcd38e0551f161d71999860bba72e52aa:b4d93efdf7899fed0944a8bd19890a72764a9e169668d4c602fc6f1199eea449” peanut.txt`. This is how I nailed down byte ordering and conversion.\\n\\nWith the right values in `w0`, `w1`, `w2`, and `w3`, we can move the *`_sxx` changes over to *`_mxx`. Just reapply your changes there while making sure the final comparison function is `COMPARE_M` instead of `COMPARE_S`.\\n\\n## Results\\n\\n{{\\u003cimg \\\"hcresult\\\" \\\"Running our new XenForo kernel.\\\" \\u003e}}\\n\\nOn an eight Nvidia 1080 GPU cracking rig from [sagitta](https://sagitta.pw/hardware/gpu-compute-nodes/brutalis/), we hit 605 MH/s with this unoptimized kernel — 7 times faster than CPU mode.\\n\\nI'll need to wait for another rainy day (and a corpus with a shorter salt) to hack on an optimized kernel.\\n\\nYou can find the final implementation at this [GitHub Gist](https://gist.github.com/tbiehn/28531fdb324baaeaa2960aae1b9b36ab).\"}","FinishReason":"stop","Err":null}
{"Request":{"model":"gpt-4","messages":[{"role":"system","content":"Digest the article and output a description of the authors persona and writing style. Preserve some unique samples of the writing to help the LLM. Provide a list of similar well-known authors and highlight key differences. This description must be useful to help LLMs emulate the authors distinct style, perspective, and philosophical impulses."},{"role":"user","content":"Article: \"\"\"Crowd-sourced, open source, and customization-out-front tools continue to make noise in the security tooling marketplace. These are not new developments. We, of course, know about projects like FindBugs and select projects from OWASP. However, new entrants like [Semgrep](https://github.com/returntocorp/semgrep) from [r2c](https://semgrep.dev/) and [Nuclei](https://github.com/projectdiscovery/nuclei) from [ProjectDiscovery](https://projectdiscovery.io/) feel different. What has attracted VC funding to these projects, and what’s behind the excitement and buzz generated from practitioners in the security space? \n\nTo dig in - and perhaps to confirm suspicions - here are the collected results from discussions with old friends who use these tools on a daily basis. These key topics drove those conversations to uncover signal:\n\n\n\n* What tools are you using? Why? How do those contrast with incumbent commercial tools?\n* What’s the best thing about using them?\n* What’s the worst thing?\n* What types of issues are they most useful for? Are these pure _defect discovery_? How do they fit into your initiative’s strategy?\n* What did you have to build or adopt to make the open sources tools useful?\n\n_Variant analysis_ was an easy idea for industry analysts to latch onto but there’s more to it than that. What makes these tools attractive, in brief, revolves around the advantages of first-class customization support, not cost. Some of the best uses for security tooling aren’t what a vendor’s product people had in mind and the best rules for that tooling can only come from tailoring to the development of real software.\n\n## Core value\n\nThe tailorability of rules (and a relatively sparse set of rules/checks that open source tools were shipped with) meant that many of these tools were first marketed as useful to a ‘variant analysis’ capability. This has given way to community efforts to adopt these tools, and private security engineering teams with no commercial interest in _owning_ checker intellectual property have freely turned them over to improve everyone’s experience of using these tools. Security and engineering groups also encourage the adoption of open or in-house risk-eliminating practices like using secure-by-default architectures. And finally, due to their flexibility, these tools are useful outside of their apparent remit of finding problems in software.\n\nVariant analysis is the elimination of a type of defect across a set of software - variant analysis usually means you’ve identified a single expression of a problem, and can generalize a check or rule for that expression across that population. An accessible example of variant analysis for Semgrep is;\n\n\n\n1. _FooBar Bank_’s login page suffered from SQL injection, the expression is the dataflow \u0026 lines of code responsible, \n2. A pattern is identified; information in GET parameters reaching a string used as a SQL query, and\n3. Semgrep is used to look for that pattern across all the Java code at _FooBar Bank_.\n\nAnd a similar example for Nuclei;\n\n\n\n1. DNS servers are compromised with guessed credentials.\n2. A pattern is identified; _FooBar Bank’s_ outsourced DNS experts keep setting up servers with the administrative password `BigFour123!`,\n3. Nuclei is used to look for SSH servers that have `root`/`BigFour123!` across _FooBar Bank_ infrastructure. (Or those lacking MFA, or privileged access management, or ...)\n\nClosely related to a variant analysis use-case is the example of sharing rules for error-prone parts of platforms or frameworks, the formats of Nuclei templates or Semgrep rules are starting to become dominant in the open exchange of ideas between industry professionals. Similar to the distribution of YARA rules by the infrastructure security set, the fast community prototyping and evaluation of Semgrep rules for the reachability of a new CVE or defect type is gaining ground. Participating in this ecosystem means you’re better able to run with the herd.\n\n{{\u003cbc \"Edsger W. Dijkstra\"\u003e}}\n...complexity sells better.\n{{\u003c/bc\u003e}}\n\nYou used to have to beg for a vendor to support finding a particular CVE, or wait for someone else to figure out the combinations of sources, sinks, and mistakes across the particular set of libraries and frameworks used by your different development teams, or deal with arcane rule languages, but now you can do this in house in reaction to bug bounties, penetration testing reports, or in-house threat modeling and development stack assessments. Increasingly these teams who work closely with real developers and real code are open sourcing the checks they create in-house, putting open source tooling more and more in direct competition with market incumbents. Anshuman mentioned that his team puts Nuclei templates into their bug bounty response process, in one case forking an existing rule in response to a bug bounty report and tailoring it to his organization’s code-bases. With that rule in place their automated tooling is able to pick up variants or regressions into the future.\n\nThese tools are also enabling one of the highest return security activities: using secure-by-default architectural components. Secure-by-default eliminates problems, and eliminated problems don’t need to be looked for, accounted for, or managed-to-remediation - yet secure-by-default remains relatively rare. Using open source tools solves a tricky element of encouraging component adoption - they can be used to detect when developers have failed to use secure-by-default approaches, and then used to nudge those teams back towards safer pastures with whatever incentives and disincentives work best for your organization. For example, maybe you want development teams to eliminate the risk of cross-site-scripting vulnerabilities by using HTML templates instead of writing raw HTML responses in _golang_. Semgrep might be used to look for instances where `http.ResponseWriter` is not being used by `template.Execute`, and then you can trigger an array of things, such as dropping the developer a slack message about how cool HTML templates are, flagging a particular build for more stringent XSS checks and security delays, or just marking a particular project as more error-prone. Tim shared an example where his developers were working on an authentication and authorization package, as they developed their secure-by-default library, they began introducing Semgrep rules that gradually removed access to functionality this library provided. Anshuman shared that as good as it is to have secure-by-default frameworks, getting developers to use them is another issue, his tooling is able to nudge developers towards safe-by-default frameworks when a push to a repository happens.\n\nMost interesting are unanticipated use cases - security tools commonly emit artifacts, have intermediate results, or have technical capabilities that could be leveraged for reasoning about populations of code and coders, if only they were unlocked. Open source tools don’t have these limitations - Semgrep and Nuclei can be used for portfolio level efforts to understand code, coding practices, and coding risk in ways that can drive interesting and effective behavior by security teams. One typical use case is software inventory and risk analysis - sure you can ask a development team if their project uses, for example, _credit card services_, but with Semgrep you can just write a query for them. With a set of checks that tell you what software properties express risk, you can realize many benefits of threat modeling in a totally automated way - use that information to automatically risk-score applications in your inventory, and respond with automation to address risk. Nuclei can similarly be used along with its Wappalyzer set to determine, experimentally, attacker-facing application stacks, which is a useful list to have when deciding what an initiative could focus on next.\n\nOne potential of these tools is teased by Alphabet’s [Tricorders](https://research.google/pubs/pub43322/) project, an effort to replace big-box static analysis tools with a largely in-house pipeline. One interesting observation was that engineers who were contributing to libraries or frameworks had begun writing and distributing their own checks and rules to enforce safe use of their APIs. Those engineers in some cases even simplified APIs to make issues easier for tooling to catch. Instead of incorporating pitfalls in documentation that developers need to read or memorize, guidance can be provided as code. Those engineers gain more agency over the safety or error-prone-ness of their own code, and seem to want to make that code less risky to build on. Tim’s firm has a solid infusion of ex-Google folks, and unsurprisingly, those engineering teams are already writing their own Semgrep checks, and his infrastructure folks incorporate automated Nuclei checks to enforce their own standards. Open source projects with simple rule languages like Nuclei and Semgrep are in a great position to become the _lingua franca_ used by modern engineers and distributed alongside popular development frameworks to realize similar dynamics outside of the halls of super-high-tech-firms.\n\n\n## Operation\n\nStarting with these tools doesn’t take much more than pulling copies down and running them against raw targets. Those squeezing good value from these tools all had to develop a few capabilities we can learn from:\n\n* Automated Execution - Tools can be instantiated ad-hoc, or by automation as part of normal or scheduled processes. You can throw them in containers and run them with docker.\n* Centralized Rules - Rules can come from anywhere, and new instances of the tools use the latest rules from the set. You can keep your rules in a Git repo, and put them into your automated container builds.\n* Environmental Integration - Tool conditions can reach affected parties natively. Asynchronous Semgrep insights wind up as bot-contributed comments on pull-requests.\n* Environmental Feedback - Impacted populations can complain about results. For example, you collect ‘not-useful’ information from bot comments, and act on that when over ten percent of flagged issues come back as not-useful for a given rule.\n\nThese capability areas have their own levels of maturity, and reflect predominant and established use-cases. They’re ‘table stakes’ before getting into nascent capabilities like automated inventory risk assessment; it probably means mashing tool output up with inventory and pursuing further integrations with governance as code elements.\n\n\n## Insights\n\nThese tools can scale an individual bit of knowledge trapped inside a security engineer across a portfolio of code, or the world’s code. Their flexibility means you can count on these tools to tell you more about your software, rather than if that software just has a particular security problem.\n\nRealizing secure by default, automated inventory, risk assessments, and threat modeling, means developing patterns and seeing what systems of incentives work best with different engineering populations. It’s a lot of work for any one organization - you’ll want to establish and cultivate ways of effectively collaborating in the commons on building these datasets.\n\nCommunity sourced checks and rules are one way to keep pace with the growth of platform, library, framework, and development paradigms. Investors have started to realize the dynamics of addressing checker and rule-set scale with closed source business models that lack credible network effects, the advantages of last-mile customizability, and the advantages of letting skilled customers that are also engineers take the products to interesting and unanticipated places.\n\nEveryone involved with security has run into burnout caused by a mismatch of commercial security tools with development practice, massive run-times because they need to reduce false positive rate in the face of outrageously risky programming practices, traps of point-solution driven penetrate and patch behavior, and those tools' inflexibility to address those problems yourself. Give yourself, your initiative, and your community of security engineers the ability to overcome these problems by investing in, becoming familiar with, and leveraging these promising open source tools.\n\n{{\u003cbc \"Alan Kay\"\u003e}}\nThe best way to predict the future is to invent it.\n{{\u003c/bc\u003e}}\n\n## Acknowledgements\n\nI’ve long been curious about how the dynamics of open source tooling have been playing out in industry, and without the illuminating conversations I’ve had with old colleagues this article would not exist.\n\n\n\n* [Anshuman Bhartiya](https://www.anshumanbhartiya.com/), AppSec specialist at a California tech firm, a history of Fortune 500 and startup experience. \n* [Ty Bross](https://twitter.com/AnInsiderThreat), AppSec and CloudSec specialist.\n* [Parsia Hakimian](https://parsiya.net/), AppSec specialist, static analysis enthusiast, and avid gamer.\n* [Tim Michaud](https://twitter.com/TimGMichaud), AppSec with a Machine Learning startup.\n* Plus one participant who wished to remain anonymous, focused on automated/scaled infrastructure security at a Fortune 500 tech company.\n\nThanks to [Sammy Migues](https://www.linkedin.com/in/sammymigues/) and [Mike Doyle](https://twitter.com/Fe3Mike) who provided extensive feedback and edits.\"\"\""}],"max_tokens":4000},"Response":"The author's persona is an expert who is knowledgeable about open-source security tools and understands the nuances of the cybersecurity industry. They display a deep understanding of the subject matter while providing a balanced perspective on the advantages and drawbacks of open-source tools. \n\nThe writing style is informative, engaging, and detailed. The author refers to specific examples and tools to help readers understand the topic. Quotes from prominent figures in the field add depth to the discussion. The article is structured with subheadings that guide the reader through the core value, insights, and operation of open-source security tools.\n\nUnique samples of writing include:\n1. \"What makes these tools attractive, in brief, revolves around the advantages of first-class customization support, not cost.\"\n2. \"Security and engineering groups also encourage the adoption of open or in-house risk-eliminating practices like using secure-by-default architectures.\"\n3. \"These tools are also enabling one of the highest return security activities: using secure-by-default architectural components.\"\n4. \"Everyone involved with security has run into burnout caused by a mismatch of commercial security tools with development practice.\"\n\nSimilar well-known authors include Bruce Schneier, Brian Krebs, and Daniel Miessler. The key differences are that the author of this article specifically explores the world of open-source security tools and their impact on the cybersecurity industry, while the other authors cover a broader range of cybersecurity topics.\n\nTo help LLMs emulate the author's distinct style, perspective, and philosophical impulses:\n\n1. Focus on providing in-depth and knowledgeable insights about the subject matter.\n2. Use specific examples and tools to illustrate points.\n3. Divide the content into well-organized sections with clear subheadings.\n4. Aim for a balanced perspective, addressing both advantages and drawbacks, while offering potential solutions.\n5. Incorporate relevant quotes from experts in the field to add credibility and authority to the discussion.","Identifier":"{\"article\":\"Crowd-sourced, open source, and customization-out-front tools continue to make noise in the security tooling marketplace. These are not new developments. We, of course, know about projects like FindBugs and select projects from OWASP. However, new entrants like [Semgrep](https://github.com/returntocorp/semgrep) from [r2c](https://semgrep.dev/) and [Nuclei](https://github.com/projectdiscovery/nuclei) from [ProjectDiscovery](https://projectdiscovery.io/) feel different. What has attracted VC funding to these projects, and what’s behind the excitement and buzz generated from practitioners in the security space? \\n\\nTo dig in - and perhaps to confirm suspicions - here are the collected results from discussions with old friends who use these tools on a daily basis. These key topics drove those conversations to uncover signal:\\n\\n\\n\\n* What tools are you using? Why? How do those contrast with incumbent commercial tools?\\n* What’s the best thing about using them?\\n* What’s the worst thing?\\n* What types of issues are they most useful for? Are these pure _defect discovery_? How do they fit into your initiative’s strategy?\\n* What did you have to build or adopt to make the open sources tools useful?\\n\\n_Variant analysis_ was an easy idea for industry analysts to latch onto but there’s more to it than that. What makes these tools attractive, in brief, revolves around the advantages of first-class customization support, not cost. Some of the best uses for security tooling aren’t what a vendor’s product people had in mind and the best rules for that tooling can only come from tailoring to the development of real software.\\n\\n## Core value\\n\\nThe tailorability of rules (and a relatively sparse set of rules/checks that open source tools were shipped with) meant that many of these tools were first marketed as useful to a ‘variant analysis’ capability. This has given way to community efforts to adopt these tools, and private security engineering teams with no commercial interest in _owning_ checker intellectual property have freely turned them over to improve everyone’s experience of using these tools. Security and engineering groups also encourage the adoption of open or in-house risk-eliminating practices like using secure-by-default architectures. And finally, due to their flexibility, these tools are useful outside of their apparent remit of finding problems in software.\\n\\nVariant analysis is the elimination of a type of defect across a set of software - variant analysis usually means you’ve identified a single expression of a problem, and can generalize a check or rule for that expression across that population. An accessible example of variant analysis for Semgrep is;\\n\\n\\n\\n1. _FooBar Bank_’s login page suffered from SQL injection, the expression is the dataflow \\u0026 lines of code responsible, \\n2. A pattern is identified; information in GET parameters reaching a string used as a SQL query, and\\n3. Semgrep is used to look for that pattern across all the Java code at _FooBar Bank_.\\n\\nAnd a similar example for Nuclei;\\n\\n\\n\\n1. DNS servers are compromised with guessed credentials.\\n2. A pattern is identified; _FooBar Bank’s_ outsourced DNS experts keep setting up servers with the administrative password `BigFour123!`,\\n3. Nuclei is used to look for SSH servers that have `root`/`BigFour123!` across _FooBar Bank_ infrastructure. (Or those lacking MFA, or privileged access management, or …)\\n\\nClosely related to a variant analysis use-case is the example of sharing rules for error-prone parts of platforms or frameworks, the formats of Nuclei templates or Semgrep rules are starting to become dominant in the open exchange of ideas between industry professionals. Similar to the distribution of YARA rules by the infrastructure security set, the fast community prototyping and evaluation of Semgrep rules for the reachability of a new CVE or defect type is gaining ground. Participating in this ecosystem means you’re better able to run with the herd.\\n\\n{{\\u003cbc \\\"Edsger W. Dijkstra\\\"\\u003e}}\\n...complexity sells better.\\n{{\\u003c/bc\\u003e}}\\n\\nYou used to have to beg for a vendor to support finding a particular CVE, or wait for someone else to figure out the combinations of sources, sinks, and mistakes across the particular set of libraries and frameworks used by your different development teams, or deal with arcane rule languages, but now you can do this in house in reaction to bug bounties, penetration testing reports, or in-house threat modeling and development stack assessments. Increasingly these teams who work closely with real developers and real code are open sourcing the checks they create in-house, putting open source tooling more and more in direct competition with market incumbents. Anshuman mentioned that his team puts Nuclei templates into their bug bounty response process, in one case forking an existing rule in response to a bug bounty report and tailoring it to his organization’s code-bases. With that rule in place their automated tooling is able to pick up variants or regressions into the future.\\n\\nThese tools are also enabling one of the highest return security activities: using secure-by-default architectural components. Secure-by-default eliminates problems, and eliminated problems don’t need to be looked for, accounted for, or managed-to-remediation - yet secure-by-default remains relatively rare. Using open source tools solves a tricky element of encouraging component adoption - they can be used to detect when developers have failed to use secure-by-default approaches, and then used to nudge those teams back towards safer pastures with whatever incentives and disincentives work best for your organization. For example, maybe you want development teams to eliminate the risk of cross-site-scripting vulnerabilities by using HTML templates instead of writing raw HTML responses in _golang_. Semgrep might be used to look for instances where `http.ResponseWriter` is not being used by `template.Execute`, and then you can trigger an array of things, such as dropping the developer a slack message about how cool HTML templates are, flagging a particular build for more stringent XSS checks and security delays, or just marking a particular project as more error-prone. Tim shared an example where his developers were working on an authentication and authorization package, as they developed their secure-by-default library, they began introducing Semgrep rules that gradually removed access to functionality this library provided. Anshuman shared that as good as it is to have secure-by-default frameworks, getting developers to use them is another issue, his tooling is able to nudge developers towards safe-by-default frameworks when a push to a repository happens.\\n\\nMost interesting are unanticipated use cases - security tools commonly emit artifacts, have intermediate results, or have technical capabilities that could be leveraged for reasoning about populations of code and coders, if only they were unlocked. Open source tools don’t have these limitations - Semgrep and Nuclei can be used for portfolio level efforts to understand code, coding practices, and coding risk in ways that can drive interesting and effective behavior by security teams. One typical use case is software inventory and risk analysis - sure you can ask a development team if their project uses, for example, _credit card services_, but with Semgrep you can just write a query for them. With a set of checks that tell you what software properties express risk, you can realize many benefits of threat modeling in a totally automated way - use that information to automatically risk-score applications in your inventory, and respond with automation to address risk. Nuclei can similarly be used along with its Wappalyzer set to determine, experimentally, attacker-facing application stacks, which is a useful list to have when deciding what an initiative could focus on next.\\n\\nOne potential of these tools is teased by Alphabet’s [Tricorders](https://research.google/pubs/pub43322/) project, an effort to replace big-box static analysis tools with a largely in-house pipeline. One interesting observation was that engineers who were contributing to libraries or frameworks had begun writing and distributing their own checks and rules to enforce safe use of their APIs. Those engineers in some cases even simplified APIs to make issues easier for tooling to catch. Instead of incorporating pitfalls in documentation that developers need to read or memorize, guidance can be provided as code. Those engineers gain more agency over the safety or error-prone-ness of their own code, and seem to want to make that code less risky to build on. Tim’s firm has a solid infusion of ex-Google folks, and unsurprisingly, those engineering teams are already writing their own Semgrep checks, and his infrastructure folks incorporate automated Nuclei checks to enforce their own standards. Open source projects with simple rule languages like Nuclei and Semgrep are in a great position to become the _lingua franca_ used by modern engineers and distributed alongside popular development frameworks to realize similar dynamics outside of the halls of super-high-tech-firms.\\n\\n\\n## Operation\\n\\nStarting with these tools doesn’t take much more than pulling copies down and running them against raw targets. Those squeezing good value from these tools all had to develop a few capabilities we can learn from:\\n\\n* Automated Execution - Tools can be instantiated ad-hoc, or by automation as part of normal or scheduled processes. You can throw them in containers and run them with docker.\\n* Centralized Rules - Rules can come from anywhere, and new instances of the tools use the latest rules from the set. You can keep your rules in a Git repo, and put them into your automated container builds.\\n* Environmental Integration - Tool conditions can reach affected parties natively. Asynchronous Semgrep insights wind up as bot-contributed comments on pull-requests.\\n* Environmental Feedback - Impacted populations can complain about results. For example, you collect ‘not-useful’ information from bot comments, and act on that when over ten percent of flagged issues come back as not-useful for a given rule.\\n\\nThese capability areas have their own levels of maturity, and reflect predominant and established use-cases. They’re ‘table stakes’ before getting into nascent capabilities like automated inventory risk assessment; it probably means mashing tool output up with inventory and pursuing further integrations with governance as code elements.\\n\\n\\n## Insights\\n\\nThese tools can scale an individual bit of knowledge trapped inside a security engineer across a portfolio of code, or the world’s code. Their flexibility means you can count on these tools to tell you more about your software, rather than if that software just has a particular security problem.\\n\\nRealizing secure by default, automated inventory, risk assessments, and threat modeling, means developing patterns and seeing what systems of incentives work best with different engineering populations. It’s a lot of work for any one organization - you’ll want to establish and cultivate ways of effectively collaborating in the commons on building these datasets.\\n\\nCommunity sourced checks and rules are one way to keep pace with the growth of platform, library, framework, and development paradigms. Investors have started to realize the dynamics of addressing checker and rule-set scale with closed source business models that lack credible network effects, the advantages of last-mile customizability, and the advantages of letting skilled customers that are also engineers take the products to interesting and unanticipated places.\\n\\nEveryone involved with security has run into burnout caused by a mismatch of commercial security tools with development practice, massive run-times because they need to reduce false positive rate in the face of outrageously risky programming practices, traps of point-solution driven penetrate and patch behavior, and those tools' inflexibility to address those problems yourself. Give yourself, your initiative, and your community of security engineers the ability to overcome these problems by investing in, becoming familiar with, and leveraging these promising open source tools.\\n\\n{{\\u003cbc \\\"Alan Kay\\\"\\u003e}}\\nThe best way to predict the future is to invent it.\\n{{\\u003c/bc\\u003e}}\\n\\n## Acknowledgements\\n\\nI’ve long been curious about how the dynamics of open source tooling have been playing out in industry, and without the illuminating conversations I’ve had with old colleagues this article would not exist.\\n\\n\\n\\n* [Anshuman Bhartiya](https://www.anshumanbhartiya.com/), AppSec specialist at a California tech firm, a history of Fortune 500 and startup experience. \\n* [Ty Bross](https://twitter.com/AnInsiderThreat), AppSec and CloudSec specialist.\\n* [Parsia Hakimian](https://parsiya.net/), AppSec specialist, static analysis enthusiast, and avid gamer.\\n* [Tim Michaud](https://twitter.com/TimGMichaud), AppSec with a Machine Learning startup.\\n* Plus one participant who wished to remain anonymous, focused on automated/scaled infrastructure security at a Fortune 500 tech company.\\n\\nThanks to [Sammy Migues](https://www.linkedin.com/in/sammymigues/) and [Mike Doyle](https://twitter.com/Fe3Mike) who provided extensive feedback and edits.\"}","FinishReason":"stop","Err":null}
{"Request":{"model":"gpt-4","messages":[{"role":"system","content":"Digest the article and output a description of the authors persona and writing style. Preserve some unique samples of the writing to help the LLM. Provide a list of similar well-known authors and highlight key differences. This description must be useful to help LLMs emulate the authors distinct style, perspective, and philosophical impulses."},{"role":"user","content":"Article: \"\"\"`CURRYFINGER` measures a vanilla request for a particular URL against requests directed to specific IP addresses with forced TLS SNI and HTTP Host headers. The tool takes a string edit distance, and emits matches according to a rough similarity metric threshold.\n\nThere are many guides that explain the process of finding servers that may actually host a CDN fronted domain, which all boil down to;\n\n * Plug the domain name into `$OSINTTool`; shodan, censys, etc.\n * Collect IP addresses.\n * ????\n * Profit\n\n## Motivation\n\n\"But Travis,\" you say \"we already have a tool for this, why do we need yet another one?\"\n\nMany guides point to an open source tool, [christophetd/CloudFlair](https://github.com/christophetd/CloudFlair), that roughly does this;\n\n * Downloads CloudFlare's IP ranges.\n * Checks whether a supplied domain resolves to an IP within those ranges.\n * Queries the Censys API for IPs serving X.509 certificates with the provided domain in the `CN=` (CommonName) attribute.\n * Loads each IP, and compares the result against a control.\n\nUnfortunately, `cloudflair.py` is a little slow, and it fails to indentify true-positives in many cases. Concretely, downloading CloudFlare's IP lists on every run compounds already slow `python` warm-up times - and, maybe more importantly, `cloudflair.py` will not work on non-CloudFlair CDNs.\n\nWhy not just commit to an existing project? One; Python has its uses, but writing highly performant multi-threaded scanners is not one of them. Two; we get value from separating the concerns of identifying targets and verifying them, to try other, more egregious methods at finding candidate origin servers than commercial OSINT platforms.\n\n`CURRYFINGER` demonstrates the kind of effective PoC you can pump out in a few hours using Golang. It has been battle tested against thousands of domains, across hundreds of thousands of requests, and run on dozens of servers. I'll share that information in another post, but let's just take a look at one example;\n\n## Head 2 Head \u0026 Demo\n\nHere, we put `cloudflair.py` up against `CURRYFINGER` in an attempt to identify the real server behind the popular \"chat\" website `chaturbate.com`. \n\n### Left Pane - CloudFlair\nWe launch `./cloudflair.py -o chatbate.txt chaturbate.com` - kicking off the process of finding targets and carrying out similarity analysis. \n\n### Right Pane - CURRYFINGER\n\nWe find targets by querying the Shodan REST API; `curl \"https://api.shodan.io/shodan/host/search?key=$SHO\u0026query=ssl%3A\\\"chaturbate.com\\\"\" | jq \".matches|.[].ip_str\" | tr -d \"\\\"\\t \" | tee chaturbate.com.txt` \n\n{{\u003caside\u003e}}\n the `jq` command selects only ip addresses from the response, and `tr` trims up quotes and whitespace. You can easily substitute Censys or a list of `masscan -p443`'d hosts here.\n{{\u003c/aside\u003e}}\n\nThen we invoke `CURRYFINGER` on the results to find which IPs seem like the real origin servers behind the CDN; `./CURRYFINGER -file chaturbate.com.txt -show=false -url https://chaturbate.com 2\u003e/dev/null | tee res.txt`\n\nThen we drop the CloudFlare IP addresses from the results; `grep ^match res.txt | grep -v 104.16|cut -d \" \" -f 2`\n\nWe finally manually examine the full response by forcing `curl` to resolve a domain with a specific IP; `curl -vik --resolve chaturbate.com:443:$IP https://chaturbate.com`\n\n### Destruction\n\n{{\u003caside\u003e}}Full screen this to see the carnage.{{\u003c/aside\u003e}}\n\n{{\u003casciinema \"267327\" \u003e}}\n\n### So What\n\n**tl;dr**; `cloudflair.py` is still running after `CURRYFINGER` completes and we've verified the results. By the time `cloudflair.py` finishes, it has failed to identify the correct server, even though _Censys found the IP, and `cloudflair.py` checked it._\n\n{{\u003caside\u003e}}Reducing `CURRYFINGER`'s timeout with `-timeout 1s` results in a, perhaps a too effective, trounce.{{\u003c/aside\u003e}}\n\n### Getting IP Addresses\nIf you have a free Shodan account, you have an API Key;\n\n{{\u003ccode \"Grab some IPs\"\u003e}}\n{{\u003chighlight sh\u003e}}\nexport SHO=[YOUR SHODAN API KEY]\nexport DOMAIN=example.com\ncurl \"https://api.shodan.io/shodan/host/search?key=$SHO\u0026query=ssl%3A\\\"$DOMAIN\\\"\" | jq \".matches|.[].ip_str\" | tr -d \"\\\"\\t \" | tee targetIPs.com.txt\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\nYou can also grab CIDR ranges for popular cloud hosting providers, and `masscan -p443` them. I'll explore this option in another article.\n\n\n### Ulimits\n\n`CURRYFINGER` does full connects, and doesn't know what your `ulimit`s are. So, juice those up before a run; `ulimit -n 60000`. Yep.\n\n\n### VHOST check; lots of domains, just a few IPs\n\nWith a pile of IP addresses in `targetIPs.com.txt` and a pile of domains in `targetDOMAINS.txt` you can quickly test for the presence of every domain on every IP by using GNU `parallel`.\n\n{{\u003ccode \"Vanilla application of GNU Parallel\"\u003e}}\n{{\u003chighlight bash\u003e}}\nparallel -j 20 ./CURRYFINGER -url https://{} -threads 200 -show=false -timeout 3s -file targetIPs.com.txt :::: targetDOMAINS.txt 2\u003e/dev/null | grep ^match | tee results.txt\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\n### All together now; match subdomains\nPull subdomains for a target domain before running `CURRYFINGER` now you're cooking with concentrated freedom. Of course, use whatever tools you want, `amass`, `subbrute`, Censys, Shodan, `masscan`, whatever.\n\n{{\u003ccode \"Set up env vars.\"\u003e}}\n{{\u003chighlight sh\u003e}}\nexport SHO=[YOUR SHODAN API KEY]\nexport DOMAIN=example.com\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\nHere's what that looks like using `turbolist3r.py`; \n\n{{\u003ccode \"Grab subdomains\"\u003e}}\n{{\u003chighlight sh\u003e}}\npython turbolist3r.py -e ssl,ask,bing,google,yahoo,netcraft,dnsdumpster,virustotal,threatcrowd,passivedns -d $DOMAIN -o targetDOMAINS.win.txt\n\n##Fix newlines...\ncat targetDOMAINS.win.txt | tr -d \"\\r\"  \u003e\u003e targetDOMAINS.txt\necho $DOMAIN\u003e\u003etargetDOMAINS.txt\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\n{{\u003ccode \"Let it rip with 400 parallel instances of CURRYFINGER and match against more bytes.\"\u003e}}\n{{\u003chighlight bash\u003e}}\ncurl \"https://api.shodan.io/shodan/host/search?key=$SHO\u0026query=ssl%3A\\\"$DOMAIN\\\"\" | jq \".matches|.[].ip_str\" | tr -d \"\\\"\\t \" | tee targetIPs.com.txt\n\nulimit -n 60000\n\nparallel -j 400 ./CURRYFINGER -url https://{} -threads 200 -show=false -timeout 3s -mbits 5000 -file targetIPs.com.txt :::: targetDOMAINS.txt 2\u003e/dev/null | grep ^match | tee results.txt\n{{\u003c/highlight\u003e}}\n{{\u003c/code\u003e}}\n\n## Grab your own copy\n\nFrom [https://github.com/tbiehn/CURRYFINGER.](https://github.com/tbiehn/CURRYFINGER)\"\"\""}],"max_tokens":4000},"Response":"The author's persona is that of a tech-savvy and straightforward individual with a good understanding of cybersecurity, networking, and software development. The writing style is informal, engaging, and easy-to-follow, with a mix of technical explanations, relevant examples, and a touch of humor. They deploy asides and code snippets throughout the article to keep the reader engaged and help them contextualize the presented information.\n\nUnique samples of the writing include:\n\n1. \"But Travis,\" you say \"we already have a tool for this, why do we need yet another one?\"\n2. \"`CURRYFINGER` demonstrates the kind of effective PoC you can pump out in a few hours using Golang.\"\n3. \"{{\u003caside\u003e}}Full screen this to see the carnage.{{\u003c/aside\u003e}}\"\n4. \"With a pile of IP addresses in `targetIPs.com.txt` and a pile of domains in `targetDOMAINS.txt` you can quickly test for the presence of every domain on every IP by using GNU `parallel`.\"\n\nSimilar well-known authors might include Brian Krebs (KrebsOnSecurity) and Bruce Schneier (schneier.com). Key differences between this author and the mentioned ones could be the higher level of technical detail and interactivity (use of code snippets) in this author's writing and their more informal and humorous style.\n\nTo emulate this author's writing style, an LLM should:\n\n1. Adopt an informal and engaging tone, making use of humor where appropriate.\n2. Utilize technical explanations and examples to convey the main points or concepts.\n3. Include code snippets and asides for reader engagement and to provide additional insights.\n4. Maintain a good balance between providing detailed explanations and keeping the content accessible and easy-to-follow.","Identifier":"{\"article\":\"`CURRYFINGER` measures a vanilla request for a particular URL against requests directed to specific IP addresses with forced TLS SNI and HTTP Host headers. The tool takes a string edit distance, and emits matches according to a rough similarity metric threshold.\\n\\nThere are many guides that explain the process of finding servers that may actually host a CDN fronted domain, which all boil down to;\\n\\n * Plug the domain name into `$OSINTTool`; shodan, censys, etc.\\n * Collect IP addresses.\\n * ????\\n * Profit\\n\\n## Motivation\\n\\n\\\"But Travis,\\\" you say \\\"we already have a tool for this, why do we need yet another one?\\\"\\n\\nMany guides point to an open source tool, [christophetd/CloudFlair](https://github.com/christophetd/CloudFlair), that roughly does this;\\n\\n * Downloads CloudFlare's IP ranges.\\n * Checks whether a supplied domain resolves to an IP within those ranges.\\n * Queries the Censys API for IPs serving X.509 certificates with the provided domain in the `CN=` (CommonName) attribute.\\n * Loads each IP, and compares the result against a control.\\n\\nUnfortunately, `cloudflair.py` is a little slow, and it fails to indentify true-positives in many cases. Concretely, downloading CloudFlare's IP lists on every run compounds already slow `python` warm-up times - and, maybe more importantly, `cloudflair.py` will not work on non-CloudFlair CDNs.\\n\\nWhy not just commit to an existing project? One; Python has its uses, but writing highly performant multi-threaded scanners is not one of them. Two; we get value from separating the concerns of identifying targets and verifying them, to try other, more egregious methods at finding candidate origin servers than commercial OSINT platforms.\\n\\n`CURRYFINGER` demonstrates the kind of effective PoC you can pump out in a few hours using Golang. It has been battle tested against thousands of domains, across hundreds of thousands of requests, and run on dozens of servers. I'll share that information in another post, but let's just take a look at one example;\\n\\n## Head 2 Head \\u0026 Demo\\n\\nHere, we put `cloudflair.py` up against `CURRYFINGER` in an attempt to identify the real server behind the popular \\\"chat\\\" website `chaturbate.com`. \\n\\n### Left Pane - CloudFlair\\nWe launch `./cloudflair.py -o chatbate.txt chaturbate.com` - kicking off the process of finding targets and carrying out similarity analysis. \\n\\n### Right Pane - CURRYFINGER\\n\\nWe find targets by querying the Shodan REST API; `curl \\\"https://api.shodan.io/shodan/host/search?key=$SHO\\u0026query=ssl%3A\\\\\\\"chaturbate.com\\\\\\\"\\\" | jq \\\".matches|.[].ip_str\\\" | tr -d \\\"\\\\\\\"\\\\t \\\" | tee chaturbate.com.txt` \\n\\n{{\\u003caside\\u003e}}\\n the `jq` command selects only ip addresses from the response, and `tr` trims up quotes and whitespace. You can easily substitute Censys or a list of `masscan -p443`'d hosts here.\\n{{\\u003c/aside\\u003e}}\\n\\nThen we invoke `CURRYFINGER` on the results to find which IPs seem like the real origin servers behind the CDN; `./CURRYFINGER -file chaturbate.com.txt -show=false -url https://chaturbate.com 2\\u003e/dev/null | tee res.txt`\\n\\nThen we drop the CloudFlare IP addresses from the results; `grep ^match res.txt | grep -v 104.16|cut -d \\\" \\\" -f 2`\\n\\nWe finally manually examine the full response by forcing `curl` to resolve a domain with a specific IP; `curl -vik --resolve chaturbate.com:443:$IP https://chaturbate.com`\\n\\n### Destruction\\n\\n{{\\u003caside\\u003e}}Full screen this to see the carnage.{{\\u003c/aside\\u003e}}\\n\\n{{\\u003casciinema \\\"267327\\\" \\u003e}}\\n\\n### So What\\n\\n**tl;dr**; `cloudflair.py` is still running after `CURRYFINGER` completes and we've verified the results. By the time `cloudflair.py` finishes, it has failed to identify the correct server, even though _Censys found the IP, and `cloudflair.py` checked it._\\n\\n{{\\u003caside\\u003e}}Reducing `CURRYFINGER`'s timeout with `-timeout 1s` results in a, perhaps a too effective, trounce.{{\\u003c/aside\\u003e}}\\n\\n### Getting IP Addresses\\nIf you have a free Shodan account, you have an API Key;\\n\\n{{\\u003ccode \\\"Grab some IPs\\\"\\u003e}}\\n{{\\u003chighlight sh\\u003e}}\\nexport SHO=[YOUR SHODAN API KEY]\\nexport DOMAIN=example.com\\ncurl \\\"https://api.shodan.io/shodan/host/search?key=$SHO\\u0026query=ssl%3A\\\\\\\"$DOMAIN\\\\\\\"\\\" | jq \\\".matches|.[].ip_str\\\" | tr -d \\\"\\\\\\\"\\\\t \\\" | tee targetIPs.com.txt\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\nYou can also grab CIDR ranges for popular cloud hosting providers, and `masscan -p443` them. I'll explore this option in another article.\\n\\n\\n### Ulimits\\n\\n`CURRYFINGER` does full connects, and doesn't know what your `ulimit`s are. So, juice those up before a run; `ulimit -n 60000`. Yep.\\n\\n\\n### VHOST check; lots of domains, just a few IPs\\n\\nWith a pile of IP addresses in `targetIPs.com.txt` and a pile of domains in `targetDOMAINS.txt` you can quickly test for the presence of every domain on every IP by using GNU `parallel`.\\n\\n{{\\u003ccode \\\"Vanilla application of GNU Parallel\\\"\\u003e}}\\n{{\\u003chighlight bash\\u003e}}\\nparallel -j 20 ./CURRYFINGER -url https://{} -threads 200 -show=false -timeout 3s -file targetIPs.com.txt :::: targetDOMAINS.txt 2\\u003e/dev/null | grep ^match | tee results.txt\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\n### All together now; match subdomains\\nPull subdomains for a target domain before running `CURRYFINGER` now you're cooking with concentrated freedom. Of course, use whatever tools you want, `amass`, `subbrute`, Censys, Shodan, `masscan`, whatever.\\n\\n{{\\u003ccode \\\"Set up env vars.\\\"\\u003e}}\\n{{\\u003chighlight sh\\u003e}}\\nexport SHO=[YOUR SHODAN API KEY]\\nexport DOMAIN=example.com\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\nHere's what that looks like using `turbolist3r.py`; \\n\\n{{\\u003ccode \\\"Grab subdomains\\\"\\u003e}}\\n{{\\u003chighlight sh\\u003e}}\\npython turbolist3r.py -e ssl,ask,bing,google,yahoo,netcraft,dnsdumpster,virustotal,threatcrowd,passivedns -d $DOMAIN -o targetDOMAINS.win.txt\\n\\n##Fix newlines...\\ncat targetDOMAINS.win.txt | tr -d \\\"\\\\r\\\"  \\u003e\\u003e targetDOMAINS.txt\\necho $DOMAIN\\u003e\\u003etargetDOMAINS.txt\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\n{{\\u003ccode \\\"Let it rip with 400 parallel instances of CURRYFINGER and match against more bytes.\\\"\\u003e}}\\n{{\\u003chighlight bash\\u003e}}\\ncurl \\\"https://api.shodan.io/shodan/host/search?key=$SHO\\u0026query=ssl%3A\\\\\\\"$DOMAIN\\\\\\\"\\\" | jq \\\".matches|.[].ip_str\\\" | tr -d \\\"\\\\\\\"\\\\t \\\" | tee targetIPs.com.txt\\n\\nulimit -n 60000\\n\\nparallel -j 400 ./CURRYFINGER -url https://{} -threads 200 -show=false -timeout 3s -mbits 5000 -file targetIPs.com.txt :::: targetDOMAINS.txt 2\\u003e/dev/null | grep ^match | tee results.txt\\n{{\\u003c/highlight\\u003e}}\\n{{\\u003c/code\\u003e}}\\n\\n## Grab your own copy\\n\\nFrom [https://github.com/tbiehn/CURRYFINGER.](https://github.com/tbiehn/CURRYFINGER)\\n\"}","FinishReason":"stop","Err":null}
